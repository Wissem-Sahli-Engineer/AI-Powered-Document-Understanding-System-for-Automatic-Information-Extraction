data collection Build a realistic, diverse dataset ,
place them raw folder : real_invoices and public contains other documents 

nb : raw_data : read-only

Normalize Image Format : Convert ALL .tif → .png 
 using python (Pillow) 
 ( creation of a function :
 convertor_tif_png( inputfolderpath , outputfolderpath, desired name for the file-"file" is default)
 )
Split Documents by ROLE

split the all folder to test-val-train using python ( splitfolder) ratio 0.7 - 0.15 -0.15

annotation : 125 invoice , Required fields.txt
( Invoice number
Date
Total amount
Currency
Supplier name (if visible) ) 

each invoice gets a JSON exemple : 
{
  "invoice_number": "INV-2024-019",
  "invoice_date": "2024-12-10",
  "total_amount": "1240.50",
  "currency": "TND",
}
store in data/annotations/ner/
nb : Filename must match image name.


PROJECT SHIFT 

install dependencies : 
pip install -r requirements.txt

Got 150 invoices from OACA they all clean adn structuted i ll start using them 
( the data is confedional is it not uploaded)

Problem 150 in 1 pdf : each page is an invoice ! 

So pdf => .png files 

using python (pdf2image ) agian create a fuction named convertor_pdf_png, downloading poppler (So python could count the pages) and copy the path of its bin ( insdie the liraby folder ) after unexrating from .zip ( downlink : https://github.com/oschwartz10612/poppler-windows/releases/ ) and split them using spliter.py

json : {
  FACTURE N°      → invoice_number
  DATE            → invoice_date
  CLIENT          → client_name
  Total Facture   → total_amount
  Devise          → currency
}
annotation 

train (35) : 1 2 3 4 5 7 9 13 17 18 ...  
test (8) : 6 8 10 12 15 16 19 26
val (7) : 11 14 24 32 43 47 50 

create json_convert to convert .png to.json ( that i will manully annoate later , NB we used natsorted so we can sort the images and took the first 50 ones)

creating a proprocces python file to convert the image to greyscale and Denoise (Clean up digital dust) ! ( i used cv2 )

text extration :
nb : Why didn’t i use bounding boxes ?
Because the invoices followed a consistent layout and high scan quality, I opted for a full-page OCR approach to simplify the pipeline and reduce error propagation, while keeping the architecture extensible for layout-based models later.

1-Choose OCR engine (important decision)
Use Tesseract OCR ( French language support is excellent + Works very well on clean scans)

Tesseract is an external program so downloading from : 
https://github.com/UB-Mannheim/tesseract/wiki

2-Extract text line by line
3-Normalize Save OCR results in a clean format

create ocr_visualizer to check if the tesserarct_ocr worked well , by drawing boxes in the image for each text ( we could see , it doesnt capture the arabic texte) ( ocr_check.png)

Data Mapping & NER Annotation 
Using a BIO (Beginning, Inside, Outside) tagging strategy, a Python mapping script aligns manual annotations (e.g., Client Name, Total Amount) with the spatial coordinates provided by Tesseract OCR.

training :
1- converting the coordinate from pixels to 0-1000 scale LayoutLM requires ! using scaler.py
Model Fine-Tuning using fine_tune_layoutlm.py
 ( Teach LayoutLMv3 to extract 5 specific financial fields from OACA invoices. ) 
 -Model: microsoft/layoutlmv3-base
 -Optimization: CUDA-enabled training on GTX 1660 Ti with FP16 precision.
 -Key Outputs: A folder named layoutlmv3-oaca containing the pytorch_model.bin (The "Brain").

 Nb : Usage: This model "reads" images and returns structured text. It requires an OCR pre-processing step for any new document.

 Final Status: Success (Halted by System Memory Spike). Model Quality: Perfect (F1: 1.0, Loss: 0.01). Resource Usage: Maxed out 6GB VRAM and triggered RAM overflow during checkpointing. Verdict: The model is ready for real-world testing.